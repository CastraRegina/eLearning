---
title: "HarvardX-PH125.9x: MovieLens Project Report"
subtitle: "Predicting user ratings from MovieLens dataset"
author: "Regina Castra"
date: "2019-03-24"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
documentclass: article
classoption: a4paper
papersize: a4
fontsize: 11pt
mainfont: FreeSans
urlcolor: blue
---


<!-- Instructions: -->
<!-- a report in the form of an Rmd file. -->
<!-- a report in the form of a PDF document knit from your Rmd file. -->
<!-- a R script or Rmd file that generates your predicted movie ratings and calculates RMSE. -->

<!-- Your movie rating predictions will be compared to the true ratings in the validation set using RMSE. -->
<!-- Be sure that your report includes the RMSE and that your R script outputs the RMSE. -->

<!-- Report -->
<!-- The report documents the analysis and presents the findings, along with supporting statistics and figures. -->
<!-- The report should include the RMSE generated.  -->

<!-- 1.) an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed -->
<!-- 2.) a methods/analysis section that explains the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and your modeling approach -->
<!-- 3.) a results section -->
<!-- 4.) a conclusion section -->

<!-- R-Script & RSME -->
<!-- The code in the R script should should be well-commented and easy to follow and is consistent with the report. -->

<!-- Report the RMSE. Target: RMSE <= 0.87750 -->
<!-- Or accuracy:  The accuracy of submission.csv is 0.95 or greater when graded using rubric.csv. -->



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




# Introductory Summary
Recommendation systems use ratings that users have given items to make specific recommendations to users.
In this project a movie recommendation system will be created, using the features in the MovieLens dataset to
predict the potential movie rating a particular user gives for a movie.
A [10M version of the MovieLens dataset](http://grouplens.org/datasets/movielens/10m/) will be used to provide a training and a validation dataset. 

The goal is to train a machine learning algorithm using the inputs in one subset to predict movie ratings in the validation set.
For a final test of the algorithm, movie ratings in the validation set have to be predicted as if they were unknown.
RMSE will be used to evaluate how close the predictions are to the true values in the validation set.
A target value of **RMSE <= 0.87750** is given.

Starting with a base model which utilizes just an average rating as a constant value, four different models taking movie and/or user effects into account are used to predict the ratings.
Two of the presented models fulfill the required target value.
The final model reaches a **RMSE** value of around **0.8648**.






\pagebreak

# Analysis

## Data Source

The
[data](http://files.grouplens.org/datasets/movielens/ml-10m.zip)
is a
[10M version of the MovieLens dataset](http://grouplens.org/datasets/movielens/10m/)
and is downloaded from 
[grouplens.org](https://grouplens.org/).
By using the provided code from the course webpage
["Create Test and Validation Sets"](https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+2T2018/jump_to/block-v1:HarvardX+PH125.9x+2T2018+type@vertical+block@e9abcdd945b1416098a15fc95807b5db) 
the data will be downloaded and separated into two datasets.



```{r LoadData, include=FALSE, eval=TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
     semi_join(edx, by = "movieId") %>%
     semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```



```{r LoadLibraries, include=FALSE, eval=TRUE}
# install and load libraries:
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret")
if(!require(knitr)) install.packages("knitr")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(grid)) install.packages("grid")
```




## Data Structure
There are two datasets provided:
**edx** as a training dataset for developing the recommendation algorithm and
**validation** for a final test of the algorithm, i.e. predict movie ratings in the validation set as if they were unknown.
Both datasets consist of `r ncol(edx)` columns,
the dataset **edx** has `r nrow(edx)` rows,
**validation** has `r nrow(validation)` rows.
The attributes are of the following types and their names are self explanatory:
```{r , echo=FALSE}
kable(data.frame(lapply(edx, FUN = function(x) typeof(x))))
```

Each row represents a rating given by one user to one movie.
```{r , echo=FALSE}
kable(head(edx, n = 5))
```

The statistic values show, that the two provided datasets are quite similar:
Statistic values of dataset **edx**:
```{r statistic_dataset_edx, echo=FALSE}
kable(summary(edx))
```

Statistic values of dataset **validation**:
```{r statistic_dataset_validation , echo=FALSE}
kable(summary(validation))
```




## Data Check
Neither dataset **edx** nor **validation** contain missing values. The number of **NA** entries in the datasets are:
```{r data_check_NA, comment="    ", echo=FALSE}
rb <- rbind(t(colSums(is.na(edx))), t(colSums(is.na(validation))))
row.names(rb) <- c("edx", "validation")
rb %>% knitr::kable()
```

The following table shows, how many distinct values of the predictors are available for the **edx** and the **validation** dataset:
```{r data_check_distinct_edx, echo=FALSE}
e <- edx %>% 
  summarize(n_userIds  = n_distinct(userId),
            n_movieIds = n_distinct(movieId),
            n_titles   = n_distinct(title),
            n_genres   = n_distinct(genres))
v <- validation %>% 
  summarize(n_userIds  = n_distinct(userId),
            n_movieIds = n_distinct(movieId),
            n_titles   = n_distinct(title),
            n_genres   = n_distinct(genres))
rb <- rbind(e,v)
row.names(rb) <- c("edx", "validation")
rb %>% knitr::kable()
```

In both datasets the number of **movieId** and **title** is not the same:
There seems to be one **title** with two **movieId**s.
It turns out that the movie with the **title** 
"`r edx %>% group_by(title,movieId) %>%
  summarize(count_title = n()) %>%
  group_by(title) %>% 
  summarize(count_movieId = n()) %>%  
  arrange(desc(count_movieId)) %>%
  slice(1) %>% 
  select(title)`"
has two different **movieId**s in the **edx** dataset as well as in the **validation** dataset: 
```{r data_check_two_movieIds, echo=FALSE}
edx %>% filter(title == "War of the Worlds (2005)") %>% 
  group_by(movieId, title, genres) %>% 
  summarize() %>% knitr::kable()
```

The next step would be to combine these two entries into one unique entry.
But it's probably not allowed to mess around with the data provided.
Therefore, the two datasets remain untouched.
In the following, a movie will be identified by its **movieId**. 
The **title** is for information only.
From now on, it is thought to have two different movies with **movieId**s 34048 and 64997, even though these two **movieId**s have the same **title**.




\pagebreak

## Data Analysis

### Data Analysis of Outcome "rating" 
Plotting a histogram of the outcome **rating** for each of the given datasets reveals that both show similar distributions.
Furthermore the half-number-ratings are less common than the whole-number-ratings.   

```{r data_analysis_dist-rating-plot, echo=FALSE, fig.height=3.5, out.width="100%", fig.show="hold"}
# Plotting the distribution of rating for training set (edx):
colorsMM<- c(Mean="darkgreen", Median="red")
p1 <- ggplot() + 
  geom_histogram(aes(edx$rating), binwidth=0.25, color="darkblue", fill="lightblue", alpha=0.5) +
  geom_vline(aes(xintercept=median(edx$rating), color="Median"), size=1.5) +
  geom_vline(aes(xintercept=mean(edx$rating), color="Mean"), size=1.5) +
  scale_colour_manual(name="Colors",values=colorsMM) +
  ggtitle("Distribution of \"rating\", edx dataset") +
  xlab("rating") +
  theme_bw() +
  theme(legend.position = c(0.20, 0.84),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'),
        legend.title = element_text(size=9, face='bold'),
        legend.text  = element_text(size=9, face='plain'))
  
# Plotting the distribution of rating for testing set (validation):
p2 <- ggplot() + 
  geom_histogram(aes(validation$rating), binwidth=0.25, color="orangered", fill="orange", alpha=0.5) +
  geom_vline(aes(xintercept=median(validation$rating), color="Median"), size=1.5) +
  geom_vline(aes(xintercept=mean(validation$rating), color="Mean"), size=1.5) +
  scale_colour_manual(name="Colors",values=colorsMM) +
  ggtitle("Distribution of \"rating\", validation dataset") + 
  xlab("rating") +
  theme_bw() +
  theme(legend.position = c(0.20, 0.84),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'),
        legend.title = element_text(size=9, face='bold'),
        legend.text  = element_text(size=9, face='plain'))
grid.arrange(p1, p2, ncol=2)
```   


The distributions show 10 distinct values of **rating**,
beginning with 0.5 in steps of 0.5 till max rating of 5 (inclusive):
```{r data_analysis_dist-rating-table, echo=FALSE}
nRatings_edx        <- edx        %>% group_by(rating) %>% arrange(rating) %>% summarize(n_edx = n())
nRatings_validation <- validation %>% group_by(rating) %>% arrange(rating) %>% summarize(n_validation = n())
merge(x = nRatings_edx, y = nRatings_validation, by = "rating", all = TRUE) %>%
  mutate("% of n_edx"        = round(n_edx       /sum(n_edx)       *100,1)) %>% 
  mutate("% of n_validation" = round(n_validation/sum(n_validation)*100,1)) %>% knitr::kable()
```




\pagebreak

### Data Analysis of Predictor "movieId"
Some movies get rated more than others as shown in a histogram:
```{r data_analysis_dist-movieId, echo=FALSE, fig.height=2.9, out.width="100%", fig.show="hold"}
edx %>% 
  count(movieId) %>%
  arrange(desc(n)) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 100, color="black", fill="lightgrey") +
  geom_vline(aes(xintercept=median(n), color="Median"), size=1.5) +
  geom_vline(aes(xintercept=mean(n), color="Mean"), size=1.5) +
  scale_colour_manual(name="Colors",values=colorsMM) +
  scale_x_log10() +
  ggtitle("Movies") +
  xlab("Number of ratings n") +
  theme_bw() +
  theme(legend.position = c(0.90, 0.80),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'),
        legend.title = element_text(size=9, face='bold'),
        legend.text  = element_text(size=9, face='plain'))
```

Following table lists the movies, which got the most ratings: 
```{r data_analysis_table-movieId-maxRatings, echo=FALSE}
edx %>% 
  group_by(movieId, title) %>%
  summarize(n = n(), avg_rating=mean(rating)) %>% 
  arrange(desc(n)) %>%
  head(9) %>% 
  knitr::kable()
```

Movies with a high number of ratings tend to have higher ratings:
```{r data_analysis_movieId_number_of_ratings, echo=FALSE, fig.height=3.3, out.width="100%", fig.show="hold"}
p <- edx %>% 
  group_by(movieId) %>% 
  summarize(avg_rating=mean(rating), n_movieId = n()) %>% 
  arrange(desc(avg_rating)) %>% 
  ggplot(aes(x=n_movieId, y=avg_rating)) +
    geom_point(shape=1) +
    geom_smooth(se=FALSE, size=2) + 
    scale_x_log10() +
    ggtitle("Movies") +
    xlab("Number of ratings n") +
    ylab("Average of ratings") +
    theme_bw() +
    theme(
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))
suppressMessages(print(p))
```




\pagebreak

### Data Analysis of Predictor "userId"
Some users are more active than others at rating movies:
```{r data_analysis_dist-userId, echo=FALSE, fig.height=2.9, out.width="100%", fig.show="hold"}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 100, color="black", fill="lightgrey") + 
  geom_vline(aes(xintercept=median(n), color="Median"), size=1.5) +
  geom_vline(aes(xintercept=mean(n), color="Mean"), size=1.5) +
  scale_colour_manual(name="Colors",values=colorsMM) +
  scale_x_log10() + 
  ggtitle("Users") +
  xlab("Number of ratings n") +
  theme_bw() +
  theme(legend.position = c(0.90, 0.80),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'),
        legend.title = element_text(size=9, face='bold'),
        legend.text  = element_text(size=9, face='plain'))
```

Looking at the users' average ratings, a substantial variability across users is noticeable:
```{r data_analysis_table-userId-avgRatings, echo=FALSE, fig.height=2.9, out.width="100%", fig.show="hold"}
edx %>% 
  group_by(userId) %>% 
  summarize(user_avg_rating=mean(rating)) %>%  
  ggplot() + 
  geom_histogram(aes(user_avg_rating), bins = 30, color="black", fill="grey", alpha=0.5)+
  geom_vline(aes(xintercept=median(user_avg_rating), color="Median"), size=1.5) +
  geom_vline(aes(xintercept=mean(user_avg_rating), color="Mean"), size=1.5) +
  scale_colour_manual(name="Colors",values=colorsMM) +
  ggtitle("Users") +
  xlab("User's average rating") +
  theme_bw() +
  theme(legend.position = c(0.10, 0.80),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'),
        legend.title = element_text(size=9, face='bold'),
        legend.text  = element_text(size=9, face='plain'))
```

There seems to be no significant correlation between average rating and number of ratings with respect of users:
```{r data_analysis_userId_number_of_ratings, echo=FALSE, fig.height=2.9, out.width="100%", fig.show="hold"}
p <- edx %>% 
  group_by(userId) %>% 
  summarize(avg_rating=mean(rating), n_userId = n()) %>% 
  arrange(desc(avg_rating)) %>% 
  ggplot(aes(x=n_userId, y=avg_rating)) +
    geom_point(shape=1) +
    geom_smooth(se=FALSE, size=2) + 
    scale_x_log10() +
    ggtitle("Users") +
    xlab("Number of ratings n") +
    ylab("Average of ratings") +
    theme_bw() +
    theme(
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))
suppressMessages(print(p))
```





\pagebreak

# Modeling
In order to compare different models a loss-function that computes the Residual Mean Squared (RMSE) is introduced.
Using $y_{u,i}$ as the rating for movie $i$ by user $u$ and denoting our prediction with $\hat{y}_{u,i}$, the RMSE is defined as
$$RMSE = \sqrt{ \frac{1}{N} \sum_{u,i}(\hat{y}_{u,i} - y_{u,i})^2 }$$
with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.
It is the typical error someone makes when predicting a movie rating.
If this number is larger than 1, it means the typical error is larger than one star.

```{r modeliing_RMSE, echo=FALSE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```



## Model 0: Use the Average of Ratings as a Baseline Model 
The simplest possible recommendation system is to predict the same rating for all movies regardless of user.
A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like
$$Y_{u,i} = \mu + \epsilon_{u,i}$$
with $\epsilon_{i}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the "true" rating for all movies.
The estimate that minimizes the RMSE is the least squares estimate of $\mu$ and, in this case, is the average of all ratings.

```{r modelingM0_avg, echo=FALSE, fig.align='center', fig.height=3.5, out.width="80%", fig.show="hold"}
mu_hat <- mean(edx$rating)

# validate the model:
rmse_m0 <- RMSE(validation$rating, mu_hat)

# store the result in a table to compare it with other models:
rmse_results <- tibble(Method = "M0: Just the average", RMSE = rmse_m0)
```

For further information, please refer to
Rafael A. Irizarry's book
[Introduction to Data Science (2019-03-17)](https://rafalab.github.io/dsbook/large-datasets.html#a-first-model).



## Model 1: Movie Effect Model
Some movies are just generally rated higher than others.
That different movies are rated differently, is confirmed by data.
The previous model can be modified by adding the term $b_{i}$ to represent average ranking for movie $i$:
$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$
Using this model the distribution of $b_{i}$ will look like the following:

```{r modelingM1_b_i, echo=FALSE, fig.align='center', fig.height=3.5, out.width="80%", fig.show="hold"}
mu <- mean(edx$rating) 

# Introduce b_i to represent average ranking for movie i, i.e. difference to overall average mu:
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# plot the distribution of b_i:
movie_avgs %>% ggplot() + 
  geom_histogram(aes(b_i), binwidth=0.5, color="black", fill="grey", alpha=0.5) +
  xlab("b_i") +
  theme_bw() + 
  theme(
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

# create predictions for test-set:
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  .$pred

# validate the model:
rmse_m1 <- RMSE(validation$rating, predicted_ratings)

# store the result in a table to compare it with other models:
rmse_results <- bind_rows(rmse_results, tibble(Method="M1: Movie Effect Model", RMSE = rmse_m1 ))
```

For further information, please refer to
Rafael A. Irizarry's book
[Introduction to Data Science (2019-03-17)](https://rafalab.github.io/dsbook/large-datasets.html#modeling-movie-effects).



\pagebreak

## Model 2: Movie + User Effects Model
As already shown in the **Analysis** chapter, there is a substantial variability across users.
To take this into account, the model can be further improved:
$$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$$
where $b_{u}$ is a user-specific effect.

```{r modelingM2_b_i_b_u, echo=FALSE, fig.align='center', fig.height=3.7, out.width="90%", fig.show="hold"}
mu <- mean(edx$rating) 

# Introduce b_i to represent average ranking for movie i, i.e. difference to overall average mu:
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# Introduce b_u to represent a user-specific effect
user_avgs <- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# create predictions for test-set:
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs,  by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

# validate the model:
rmse_m2 <- RMSE(validation$rating, predicted_ratings)

# store the result in a table to compare it with other models:
rmse_results <- bind_rows(rmse_results, tibble(Method="M2: Movie + User Effects Model", RMSE = rmse_m2 ))
```

For further information, please refer to
Rafael A. Irizarry's book
[Introduction to Data Science (2019-03-17)](https://rafalab.github.io/dsbook/large-datasets.html#user-effects).



## Model 3: Regularized Movie Effect Model
Looking at the data it can be shown that some movies are only rated by few users.
Some of these users gave movies very high or very low rates.
With just a few users, we have more uncertainty.
Therefore, larger estimates of $b_{i}$, negative or positive, are more likely.
By using regularization it is possible to penalize large estimates that are formed using small sample sizes.
It is intended to fit the model
$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$
Instead of minimizing the least square equation, an equation that adds a penalty will be minimized:
$$\frac{1}{N}\sum_{u,i}(y_{u,i} - \mu - b_{i})^2 + \lambda\sum_{i}b_{i}^2$$
The first term is just least squares and the second is a penalty that gets larger when many $b_{i}$ are large. 
By using calculus it can actually been shown that the values of $b_{i}$ that minimize this equation are:
$$\hat{b_{i}}(\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}}(Y_{u,i}-\hat{\mu})$$
where $n_{i}$ is the number of ratings made for movie $i$ and $\lambda$ is the penalty.
Penalty $\lambda$ is a tuning parameter.
It can be determined by using cross validation just on the dataset **edx**.
Therefore the dataset **edx** is devided into a train set (90%) and a test set (10%).
Following plot shows the resulting RMSEs for several $\lambda$ used on the test set. 

```{r modelingM3_regb_i, echo=FALSE, fig.align='center', fig.height=3.7, out.width="90%", fig.show="hold"}
# Implement Model 3: Regularized Movie Effect Model
rmse_modelM3 <- function(lambda, train_set, test_set){
  mu <- mean(train_set$rating)
  
  just_the_sum <- train_set %>%
    group_by(movieId) %>%
    summarize(s = sum(rating - mu), n_i = n())

  predicted_ratings <- test_set %>%
    left_join(just_the_sum, by='movieId') %>%
    mutate(b_i = s/(n_i+lambda)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred

  RMSE(test_set$rating, predicted_ratings)  
}

# For cross-validation the dataset edx has to be devided into train_set and test_set: 
set.seed(755)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set  <- edx[-test_index,]
test_set   <- edx[ test_index,]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Iterate lambda using following range&steps:
lambdas <- seq(0, 5.75, 0.25)

# validate the model for several lambdas
rmses <- sapply(lambdas, function(lambda){
  rmse_modelM3(lambda, train_set, test_set)
})

# plot the results:
tibble(lambdas=lambdas, rmses=rmses) %>% 
  ggplot(aes(x=lambdas, y=rmses)) +
    geom_point(shape=19) +
    xlab("lambda") +
    ylab("RMSE") +
    ggtitle("Cross validation on dataset edx") +
    theme_bw() +
    theme(plot.title   = element_text(size=9, face='bold'),
          axis.title.x = element_text(size=9, face='bold'),
          axis.title.y = element_text(size=9, face='bold'))

# retrieve the best lambda:
lambda <- lambdas[which.min(rmses)]
```

\pagebreak

For dataset **edx** best value of RMSE can be achieved with $\lambda =$ `r lambda`.
Using this value, a plot of the regularized estimates versus the least squares estimates will show how the estimates shrink:

```{r modelingM3_regb_i_regularisation, echo=FALSE, fig.align='center', fig.height=3.5, out.width="90%", fig.show="hold"}
mu <- mean(edx$rating) 

# For plotting calculate the the tibbles movie_avg with original b_i and movie_reg_avgs with regularized b_i.
# Introduce b_i to represent average ranking for movie i, i.e. difference to overall average mu:
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# Here the regularized b_i are stored:
movie_reg_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())

# plot of the regularized estimates versus the original estimates.
tibble(original = movie_avgs$b_i,
       regularlized = movie_reg_avgs$b_i,
       n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) +
  geom_point(shape=1, alpha=0.5) +
  xlab("original b_i") +
  ylab("regularized b_i") +
  theme_bw() +
  theme(plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

# validate the model:
rmse_m3 <- rmse_modelM3(lambda, edx, validation)

# store the result in a table to compare it with other models:
rmse_results <- bind_rows(rmse_results, tibble(Method="M3: Regularized Movie Effect Model", RMSE = rmse_m3 ))
```

For further information, please refer to
Rafael A. Irizarry's book
[Introduction to Data Science (2019-03-17)](https://rafalab.github.io/dsbook/large-datasets.html#penalized-least-squares).



## Model 4: Regularized Movie + User Effect Model
Regularization can be used for the estimate of user effects as well. Following equation has to be minimized:
$$\frac{1}{N}\sum_{u,i}(y_{u,i} - \mu - b_{i} - b_{u})^2 + \lambda ( \sum_{i}b_{i}^2 + \sum_{u}b_{u}^2 ) $$
By cross validation on the dataset **edx** a different $\lambda$ delivers optimum RMSE:  

```{r modelingM4_regb_i_b_u, echo=FALSE, fig.align='center', fig.height=3.7, out.width="90%", fig.show="hold"}
# Implement Model 4: Regularized Movie + User Effect Model:
rmse_modelM4 <- function(lambda, train_set, test_set){
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(test_set$rating, predicted_ratings))
}

# For cross-validation the dataset edx has to be devided into train_set and test_set: 
set.seed(755)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set  <- edx[-test_index,]
test_set   <- edx[ test_index,]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Iterate lambda using following range&steps:
lambdas <- seq(0, 12, 0.25)

# validate the model for several lambdas
rmses <- sapply(lambdas, function(lambda){
  rmse_modelM4(lambda, train_set, test_set)
})

# plot the results:
tibble(lambdas=lambdas, rmses=rmses) %>% 
  ggplot(aes(x=lambdas, y=rmses)) +
  geom_point(shape=19) +
  xlab("lambda") +
  ylab("RMSE") +
  ggtitle("Cross validation on dataset edx") +
  theme_bw() +
  theme(plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

# retrieve the best lambda:
lambda <- lambdas[which.min(rmses)]

# validate the model:
rmse_m4 <- rmse_modelM4(lambda, edx, validation)

# store the result in a table to compare it with other models:
rmse_results <- bind_rows(rmse_results, tibble(Method="M4: Regularized Movie + User Effect Model", RMSE = rmse_m4 ))

# Best method is:
bestRMSE <- rmse_results[which.min(rmse_results$RMSE),] 

```

For dataset **edx** best value of RMSE can be achieved with $\lambda =$ `r lambda` for this kind of model. 

For further information, please refer to
Rafael A. Irizarry's book
[Introduction to Data Science (2019-03-17)](https://rafalab.github.io/dsbook/large-datasets.html#choosing-the-penalty-terms).




\pagebreak

# Results

Using above models on the **validation** dataset leads to following results:

```{r results, echo=FALSE, fig.align='center', fig.height=3.7, out.width="90%", fig.show="hold"}
rmse_results %>% knitr::kable()
```

Two Models create RMSE values below the given target value of **RMSE <= 0.87750**.
In both cases movie and user effects are taken into account.

The **`r bestRMSE$Method`** achieves the lowest **RMSE** value of around **`r round(bestRMSE$RMSE, digits =4)`**.




<!-- \pagebreak -->

# Conclusion
In this project a movie recommendation system was created, using the features in the **MovieLens** dataset to predict
the potential movie rating a particular user gives for a movie.
A machine learning algorithm was implemented using a model which takes movie and user effects into account and predicts movie ratings.
The model which best fulfilled the given target achieves a **RMSE** value of around **`r round(bestRMSE$RMSE, digits =4)`**.

It is likely that much more sophisticated methods would achieve better RMSE values.
For example genre effects as well as the release year of the movie could be considered. 
A first analysis presented in the **Appendix** chapter show that these features would possibly improve the recommendation algorithm.
The implementation is left to future work.




\pagebreak

# Appendix

## Analysis of Genre Effects
Even if the **genre**s were not considered for the recommendation modeling, it is worth to check, if they can be used as a predictor:

``` {r appendix_genre_dist, echo=FALSE, fig.align='center', fig.height=4, out.width="90%", fig.show="hold"} 
# create statistic infos of genres:
stats_genres <- edx %>% 
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(countRating=n(), meanRating=mean(rating), countMovies=n_distinct(movieId))

# plot genre statistics: 
stats_genres %>%
  ggplot(aes(x = reorder(genres, -countMovies), y=countMovies)) +
  geom_bar(stat="identity", colour = "black", fill = "lightgrey") +
  labs(title = "Genres", x = "Genres", y = "Number of distinct movies") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust=0.25, hjust = 1),
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))
```

The distribution shows that movies are tagged in many cases with the genres Drama and/or Comedy.
Therefore it is no wonder that movies belonging to these two genres also got the largest number of ratings:

``` {r appendix_genre_statistics, echo=FALSE, fig.align='center', fig.height=4, out.width="90%", fig.show="hold"} 
p1 <- stats_genres %>% 
  ggplot(aes(x = reorder(genres, -countRating), y=countRating)) +
  geom_bar(stat="identity", colour = "black", fill = "lightgrey") +
  labs(title = "Genres", x = "Genres", y = "Number of ratings") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust=0.25, hjust = 1), 
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

p2 <- stats_genres %>% 
  ggplot(aes(x = reorder(genres, -meanRating), y=meanRating)) +
  geom_bar(stat="identity", colour = "black", fill = "lightgrey") +
  labs(title = "Genres", x = "Genres", y = "Average of ratings") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust=0.25, hjust = 1), 
        plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

grid.arrange(p1, p2, ncol=2)
```
There is a connection of "average of ratings" and the genres which probably could be used to improve the recommendation algorithm.  



\pagebreak

## Analysis of Release Year Effects
The movie title also contains the information of the release year.
This information is not yet considered for the recommendation modeling.

``` {r appendix_releaseyear_dist, echo=FALSE, fig.align='center', fig.height=9, out.width="90%", fig.show="hold"} 
edx_releaseyear <- edx %>% 
  extract(title, c("releaseyear"), regex = ".*\\s\\((\\d+)\\)", convert = TRUE, remove = FALSE) 

stats_releaseyear <- edx_releaseyear %>% 
  group_by(releaseyear) %>%
  summarize(countRating=n(), meanRating=mean(rating), countMovies=n_distinct(movieId))

p1 <- stats_releaseyear %>%
  ggplot(aes(x = releaseyear, y=countMovies)) +
  geom_bar(stat="identity", colour = "black", fill = "lightgrey") +
  labs(title = "Release Year", x = "Release Year", y = "Number of released movies each year\n") +
  theme_bw() +
  theme(plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

p2 <- stats_releaseyear %>% 
  ggplot() + 
  geom_line(aes(x = releaseyear, y = countRating))+ 
  labs(x = "Release Year", y = "Number of ratings per release year\n") +
  theme_bw() +
  theme(plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))
  
p3 <- stats_releaseyear %>% 
  ggplot(aes(x = releaseyear, y = meanRating)) +
  geom_point(shape=1) +
  geom_smooth(se=FALSE, size=2) +
  labs(x = "Release Year", y = "Average of ratings each release year\n") +
  theme_bw() +
  theme(plot.title   = element_text(size=9, face='bold'),
        axis.title.x = element_text(size=9, face='bold'),
        axis.title.y = element_text(size=9, face='bold'))

library(grid)
grid.newpage()
suppressMessages(grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), ggplotGrob(p3), size = "last")))
```

It turns out that since 1940 the newer the movie the lower its rating.
This insight could be used to improve the recommendation algorithm. 



## Sessioninfo
```{r , echo=FALSE, size = 'tiny'}
sessionInfo()
```
